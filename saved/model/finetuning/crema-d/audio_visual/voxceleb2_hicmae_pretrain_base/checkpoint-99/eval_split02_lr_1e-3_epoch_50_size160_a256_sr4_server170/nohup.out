/usr/local/lib/python3.10/dist-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
Arg GPu
3
Traceback (most recent call last):
  File "/content/drive/MyDrive/HiCMAE/run_class_finetuning_av.py", line 760, in <module>
    main(opts, ds_init)
  File "/content/drive/MyDrive/HiCMAE/run_class_finetuning_av.py", line 231, in main
    utils.init_distributed_mode(args)
  File "/content/drive/MyDrive/HiCMAE/utils.py", line 296, in init_distributed_mode
    torch.cuda.set_device(args.gpu)
  File "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py", line 313, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Arg GPu
0
| distributed init (rank 0): env://, gpu 0
Arg GPu
2
Traceback (most recent call last):
  File "/content/drive/MyDrive/HiCMAE/run_class_finetuning_av.py", line 760, in <module>
    main(opts, ds_init)
  File "/content/drive/MyDrive/HiCMAE/run_class_finetuning_av.py", line 231, in main
    utils.init_distributed_mode(args)
  File "/content/drive/MyDrive/HiCMAE/utils.py", line 296, in init_distributed_mode
    torch.cuda.set_device(args.gpu)
  File "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py", line 313, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Arg GPu
1
Traceback (most recent call last):
  File "/content/drive/MyDrive/HiCMAE/run_class_finetuning_av.py", line 760, in <module>
    main(opts, ds_init)
  File "/content/drive/MyDrive/HiCMAE/run_class_finetuning_av.py", line 231, in main
    utils.init_distributed_mode(args)
  File "/content/drive/MyDrive/HiCMAE/utils.py", line 296, in init_distributed_mode
    torch.cuda.set_device(args.gpu)
  File "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py", line 313, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 5403 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 5404) of binary: /usr/bin/python3
